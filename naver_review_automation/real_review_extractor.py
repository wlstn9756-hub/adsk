import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import logging
import time
import os
from typing import Tuple, Optional

logger = logging.getLogger(__name__)

class RealNaverReviewExtractor:
    """ì‹¤ì œ ë„¤ì´ë²„ ë¦¬ë·° ì¶”ì¶œê¸° - Chrome ìˆìœ¼ë©´ Selenium, ì—†ìœ¼ë©´ HTTP"""
    
    def __init__(self):
        self.driver = None
        self.chrome_available = False
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
    def setup_selenium(self):
        """ì…€ë ˆë‹ˆì›€ ì„¤ì • (ë¡œì»¬ì—ì„œë§Œ ì‹œë„)"""
        try:
            options = webdriver.ChromeOptions()
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--disable-blink-features=AutomationControlled')
            
            # í•­ìƒ headless ëª¨ë“œë¡œ ì‹¤í–‰ (ì°½ì´ ì•ˆ ëœ¨ë„ë¡)
            options.add_argument('--headless')
            options.add_argument('--disable-gpu')
                
            options.add_experimental_option("excludeSwitches", ["enable-automation"])
            options.add_experimental_option('useAutomationExtension', False)
            
            self.driver = webdriver.Chrome(options=options)
            self.driver.implicitly_wait(5)
            self.chrome_available = True
            
            logger.info("âœ… Chrome ë“œë¼ì´ë²„ ì„¤ì • ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.warning(f"Chrome ì„¤ì • ì‹¤íŒ¨: {e}")
            logger.info("HTTP ë°©ì‹ìœ¼ë¡œ ëŒ€ì²´ ì‹¤í–‰")
            self.chrome_available = False
            return False

    def extract_direct_review_selenium(self, url: str) -> Tuple[str, str]:
        """Seleniumìœ¼ë¡œ ê°œë³„ ë¦¬ë·° í˜ì´ì§€ ì¶”ì¶œ"""
        try:
            if not self.driver:
                if not self.setup_selenium():
                    logger.error("Selenium ì„¤ì • ì‹¤íŒ¨, HTTP ë°©ì‹ìœ¼ë¡œ ì „í™˜")
                    return self.extract_with_http(url)

            logger.info(f"ê°œë³„ ë¦¬ë·° í˜ì´ì§€ ë¡œë”©: {url}")
            self.driver.get(url)
            time.sleep(4)  # ë¡œë”© ëŒ€ê¸° ì‹œê°„ ì¦ê°€

            soup = BeautifulSoup(self.driver.page_source, 'html.parser')

            # ë¦¬ë·° ë³¸ë¬¸ ì¶”ì¶œ - data-pui-click-code="reviewend.text"
            review_text = ""
            review_elem = soup.find('a', {'data-pui-click-code': 'reviewend.text'})
            if review_elem:
                review_text = review_elem.get_text(strip=True)
                logger.info(f"ë¦¬ë·° ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ: {review_text[:50]}...")
            else:
                # ëŒ€ì²´ ì„ íƒì ì‹œë„
                review_div = soup.find('div', class_='pui__vn15t2')
                if review_div:
                    review_text = review_div.get_text(strip=True)
                    logger.info(f"ëŒ€ì²´ ì„ íƒìë¡œ ë¦¬ë·° ë³¸ë¬¸ ì¶”ì¶œ: {review_text[:50]}...")
                else:
                    logger.warning("ë¦¬ë·° ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            # ì˜ìˆ˜ì¦ ë‚ ì§œ ì¶”ì¶œ - time íƒœê·¸
            receipt_date = ""
            time_elem = soup.find('time', {'aria-hidden': 'true'})
            if time_elem:
                receipt_date = time_elem.get_text(strip=True)
                logger.info(f"ì˜ìˆ˜ì¦ ë‚ ì§œ ì¶”ì¶œ: {receipt_date}")
            else:
                logger.warning("ì˜ìˆ˜ì¦ ë‚ ì§œ time íƒœê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            return (
                review_text or "ë¦¬ë·° ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                receipt_date or "ì˜ìˆ˜ì¦ ë‚ ì§œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            )

        except Exception as e:
            logger.error(f"Selenium ì¶”ì¶œ ì˜¤ë¥˜ ìƒì„¸: {type(e).__name__} - {str(e)}", exc_info=True)
            return f"Selenium ì¶”ì¶œ ì‹¤íŒ¨: {type(e).__name__}", "ë‚ ì§œ ì¶”ì¶œ ì‹¤íŒ¨"

    def extract_list_review_selenium(self, url: str, shop_name: str) -> Tuple[str, str]:
        """Seleniumìœ¼ë¡œ ë¦¬ë·° ëª©ë¡ì—ì„œ íŠ¹ì • ì—…ì²´ ë¦¬ë·° ì¶”ì¶œ"""
        try:
            if not self.driver:
                if not self.setup_selenium():
                    logger.error("Selenium ì„¤ì • ì‹¤íŒ¨, HTTP ë°©ì‹ìœ¼ë¡œ ì „í™˜")
                    return self.extract_with_http(url)

            logger.info(f"í˜ì´ì§€ ë¡œë”© ì‹œì‘: {url}")
            self.driver.get(url)

            # ë‹¨ì¶• URL ë¦¬ë””ë ‰ì…˜ ëŒ€ê¸°
            if "naver.me" in url:
                try:
                    WebDriverWait(self.driver, 10).until(lambda d: d.current_url != url)
                    logger.info(f"ë¦¬ë””ë ‰ì…˜ ì™„ë£Œ: {self.driver.current_url}")
                except Exception as e:
                    logger.error(f"ë¦¬ë””ë ‰ì…˜ ëŒ€ê¸° ì‹¤íŒ¨: {e}")

            time.sleep(4)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì‹œê°„ ì¦ê°€

            # ì—…ì²´ëª…ìœ¼ë¡œ ë¦¬ë·° ì°¾ê¸°
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            target_review = None

            review_blocks = soup.find_all('div', class_='hahVh2')
            logger.info(f"ë¦¬ë·° ë¸”ë¡ {len(review_blocks)}ê°œ ë°œê²¬")

            if len(review_blocks) == 0:
                logger.error("ë¦¬ë·° ë¸”ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í˜ì´ì§€ êµ¬ì¡°ê°€ ë³€ê²½ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                return "ë¦¬ë·° ë¸”ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "ë‚ ì§œ ì¶”ì¶œ ë¶ˆê°€"

            # ë¨¼ì € ëª¨ë“  ì—…ì²´ëª… ë¡œê¹…
            all_shop_names = []
            for block in review_blocks:
                shop_elem = block.find('span', class_='pui__pv1E2a')
                if shop_elem:
                    shop_text = shop_elem.text.strip()
                    all_shop_names.append(shop_text)
                    if shop_text == shop_name:
                        target_review = block
                        logger.info(f"ì—…ì²´ëª… '{shop_name}' ë§¤ì¹­ ì„±ê³µ")
                        break

            logger.info(f"ë°œê²¬ëœ ì—…ì²´ëª…ë“¤: {all_shop_names[:5]}")

            # ìŠ¤í¬ë¡¤í•´ì„œ ë” ì°¾ê¸°
            if not target_review:
                logger.info(f"ì²« í˜ì´ì§€ì—ì„œ '{shop_name}' ë¯¸ë°œê²¬, ìŠ¤í¬ë¡¤ ì‹œì‘")
                for i in range(5):
                    self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                    time.sleep(2)

                    soup = BeautifulSoup(self.driver.page_source, 'html.parser')
                    review_blocks = soup.find_all('div', class_='hahVh2')

                    for block in review_blocks:
                        shop_elem = block.find('span', class_='pui__pv1E2a')
                        if shop_elem and shop_elem.text.strip() == shop_name:
                            target_review = block
                            logger.info(f"ìŠ¤í¬ë¡¤ {i+1}íšŒì°¨ì—ì„œ ì—…ì²´ëª… ë°œê²¬")
                            break

                    if target_review:
                        break

            # ë¦¬ë·° ë°ì´í„° ì¶”ì¶œ
            if target_review:
                # ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì‹œë„
                try:
                    more_button = target_review.find('a', {'data-pui-click-code': 'otherreviewfeed.rvshowmore'})
                    if more_button:
                        selenium_blocks = self.driver.find_elements(By.CSS_SELECTOR, "div.hahVh2")
                        for selenium_block in selenium_blocks:
                            if shop_name in selenium_block.text:
                                try:
                                    more_btn = selenium_block.find_element(By.CSS_SELECTOR, "a[data-pui-click-code='otherreviewfeed.rvshowmore']")
                                    self.driver.execute_script("arguments[0].click();", more_btn)
                                    time.sleep(1)
                                    soup = BeautifulSoup(self.driver.page_source, 'html.parser')
                                    # ë‹¤ì‹œ íƒ€ê²Ÿ ë¦¬ë·° ì°¾ê¸°
                                    review_blocks = soup.find_all('div', class_='hahVh2')
                                    for block in review_blocks:
                                        shop_elem = block.find('span', class_='pui__pv1E2a')
                                        if shop_elem and shop_elem.text.strip() == shop_name:
                                            target_review = block
                                            break
                                    break
                                except Exception as inner_e:
                                    logger.warning(f"ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨: {inner_e}")
                except Exception as e:
                    logger.warning(f"ë”ë³´ê¸° ë²„íŠ¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

                # ë¦¬ë·° ë³¸ë¬¸ ì¶”ì¶œ
                review_text = ""
                review_div = target_review.find('div', class_='pui__vn15t2')
                if review_div:
                    review_text = review_div.text.strip()
                    logger.info(f"ë¦¬ë·° ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ: {review_text[:50]}...")
                else:
                    logger.warning("ë¦¬ë·° ë³¸ë¬¸ divë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

                # ì˜ìˆ˜ì¦ ë‚ ì§œ ì¶”ì¶œ
                receipt_date = ""
                time_elem = target_review.find('time', {'aria-hidden': 'true'})
                if time_elem:
                    receipt_date = time_elem.text.strip()
                    logger.info(f"ì˜ìˆ˜ì¦ ë‚ ì§œ ì¶”ì¶œ: {receipt_date}")
                else:
                    logger.warning("ì˜ìˆ˜ì¦ ë‚ ì§œ time íƒœê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

                return (
                    review_text or "ë¦¬ë·° ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                    receipt_date or "ì˜ìˆ˜ì¦ ë‚ ì§œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                )
            else:
                logger.error(f"ì—…ì²´ëª… '{shop_name}'ê³¼ ì¼ì¹˜í•˜ëŠ” ë¦¬ë·°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë°œê²¬ëœ ì—…ì²´ëª…: {all_shop_names[:3]}")
                return f"ì—…ì²´ëª… '{shop_name}'ê³¼ ì¼ì¹˜í•˜ëŠ” ë¦¬ë·°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "ë‚ ì§œ ì •ë³´ ì—†ìŒ"

        except Exception as e:
            logger.error(f"ë¦¬ë·° ëª©ë¡ ì¶”ì¶œ ì˜¤ë¥˜ ìƒì„¸: {type(e).__name__} - {str(e)}", exc_info=True)
            return f"ì¶”ì¶œ ì˜¤ë¥˜ ë°œìƒ: {type(e).__name__}", "ë‚ ì§œ ì¶”ì¶œ ì‹¤íŒ¨"

    def extract_with_http(self, url: str) -> Tuple[str, str]:
        """HTTP ìš”ì²­ìœ¼ë¡œ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ (Chrome ì—†ì„ ë•Œ)"""
        try:
            logger.info(f"HTTP ë°©ì‹ìœ¼ë¡œ í˜ì´ì§€ ì ‘ê·¼: {url}")
            
            # User-Agentë¥¼ ë‹¤ì–‘í•˜ê²Œ ì‹œë„
            headers_list = [
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
                'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36'
            ]
            
            for user_agent in headers_list:
                try:
                    self.session.headers.update({'User-Agent': user_agent})
                    response = self.session.get(url, timeout=10)
                    
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.content, 'html.parser')
                        
                        # í˜ì´ì§€ ì œëª©ì—ì„œ ì—…ì²´ ì •ë³´ ì¶”ì¶œ
                        title = soup.find('title')
                        title_text = title.get_text() if title else ""
                        
                        # ë©”íƒ€ íƒœê·¸ì—ì„œ ì„¤ëª… ì¶”ì¶œ
                        meta_desc = soup.find('meta', attrs={'name': 'description'})
                        description = meta_desc.get('content', '') if meta_desc else ""
                        
                        # ê¸°ë³¸ ì •ë³´ ì¡°í•©
                        extracted_info = f"í˜ì´ì§€ ì œëª©: {title_text}\\n"
                        if description:
                            extracted_info += f"ì„¤ëª…: {description[:100]}..."
                        
                        # URL íƒ€ì…ë³„ ì •ë³´ ì¶”ê°€
                        if "/my/review/" in url:
                            extracted_info += "\\n[ê°œë³„ ë¦¬ë·° í˜ì´ì§€]"
                            date_info = "ê°œë³„ ë¦¬ë·° ë‚ ì§œ"
                        else:
                            extracted_info += "\\n[ë¦¬ë·° í”¼ë“œ í˜ì´ì§€]"
                            date_info = "í”¼ë“œ í˜ì´ì§€ ë‚ ì§œ"
                        
                        return extracted_info, date_info
                        
                except Exception as e:
                    logger.warning(f"User-Agent {user_agent} ì‹¤íŒ¨: {e}")
                    continue
            
            return "HTTP ì¶”ì¶œ ì‹¤íŒ¨ - ëª¨ë“  User-Agent ì‹œë„ ì™„ë£Œ", "ë‚ ì§œ ì¶”ì¶œ ë¶ˆê°€"
            
        except Exception as e:
            logger.error(f"HTTP ì¶”ì¶œ ì´ ì‹¤íŒ¨: {e}")
            return f"HTTP ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}", "ì˜¤ë¥˜ ë°œìƒ"

    def extract_review(self, url: str, shop_name: Optional[str] = None) -> Tuple[str, str, dict]:
        """ë©”ì¸ ì¶”ì¶œ í•¨ìˆ˜ - ìµœì ì˜ ë°©ë²• ìë™ ì„ íƒ"""
        start_time = time.time()

        try:
            logger.info(f"ë¦¬ë·° ì¶”ì¶œ ì‹œì‘: {url}")
            logger.info(f"Chrome ì‚¬ìš© ê°€ëŠ¥: {self.chrome_available}")

            # URL íŒ¨í„´ í™•ì¸ - ë„¤ì´ë²„ ë¦¬ë·° URL í˜•ì‹ë“¤
            # 1. ê°œë³„ ë¦¬ë·°: /my/review/ ë˜ëŠ” /place/review/ugc/
            # 2. ë¦¬ë·° ëª©ë¡: naver.me ë‹¨ì¶• URL ë˜ëŠ” /place/feed/
            is_direct_review = (
                "/my/review/" in url or
                "/place/review/ugc/" in url or
                "/restaurant/" in url and "review" in url
            )

            if is_direct_review:
                # ê°œë³„ ë¦¬ë·° í˜ì´ì§€
                logger.info("ê°œë³„ ë¦¬ë·° í˜ì´ì§€ ì²˜ë¦¬")
                if self.chrome_available:
                    review_text, receipt_date = self.extract_direct_review_selenium(url)
                else:
                    review_text, receipt_date = self.extract_with_http(url)
            else:
                # ë¦¬ë·° ëª©ë¡ í˜ì´ì§€ (naver.me, feed ë“±)
                logger.info("ë¦¬ë·° ëª©ë¡ í˜ì´ì§€ ì²˜ë¦¬")
                if not shop_name:
                    return "ì—…ì²´ëª…ì´ í•„ìš”í•©ë‹ˆë‹¤", "ì—…ì²´ëª… ëˆ„ë½", {}

                if self.chrome_available:
                    review_text, receipt_date = self.extract_list_review_selenium(url, shop_name)
                else:
                    review_text, receipt_date = self.extract_with_http(url)
            
            # ë©”íƒ€ë°ì´í„° ìƒì„±
            processing_time = round(time.time() - start_time, 2)
            metadata = {
                "processing_time_seconds": processing_time,
                "extraction_method": "selenium" if self.chrome_available else "http",
                "url_type": "direct" if is_direct_review else "list",
                "shop_name": shop_name,
                "success": "ì˜¤ë¥˜" not in review_text and "ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" not in review_text
            }
            
            logger.info(f"ì¶”ì¶œ ì™„ë£Œ - ì†Œìš”ì‹œê°„: {processing_time}ì´ˆ")
            return review_text, receipt_date, metadata
            
        except Exception as e:
            logger.error(f"ë¦¬ë·° ì¶”ì¶œ ì´ ì˜¤ë¥˜: {e}")
            return f"ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "ì˜¤ë¥˜ ë°œìƒ", {
                "processing_time_seconds": round(time.time() - start_time, 2),
                "extraction_method": "failed",
                "error": str(e)
            }
    
    def test_extraction_capability(self) -> dict:
        """ì¶”ì¶œ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
        try:
            # Chrome ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í…ŒìŠ¤íŠ¸
            chrome_test = self.setup_selenium()
            
            # HTTP ìš”ì²­ í…ŒìŠ¤íŠ¸
            try:
                response = self.session.get("https://www.naver.com", timeout=5)
                http_test = response.status_code == 200
            except:
                http_test = False
            
            return {
                "chrome_available": chrome_test,
                "http_available": http_test,
                "recommended_method": "selenium" if chrome_test else "http",
                "capabilities": {
                    "direct_review_extraction": chrome_test,
                    "list_review_extraction": chrome_test,
                    "basic_page_info": http_test
                }
            }
        except Exception as e:
            return {
                "chrome_available": False,
                "http_available": False,
                "error": str(e)
            }
    
    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.driver:
            self.driver.quit()
            self.driver = None

    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()

# ì „ì—­ ì¶”ì¶œê¸° ì¸ìŠ¤í„´ìŠ¤ (ì„±ëŠ¥ ìµœì í™”)
_global_extractor = None

def get_extractor():
    """ì „ì—­ ì¶”ì¶œê¸° ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
    global _global_extractor
    if _global_extractor is None:
        _global_extractor = RealNaverReviewExtractor()
        _global_extractor.setup_selenium()  # ë¯¸ë¦¬ ì„¤ì •
    return _global_extractor

def extract_naver_review_real(url: str, shop_name: Optional[str] = None) -> Tuple[str, str, dict]:
    """ì‹¤ì œ ë„¤ì´ë²„ ë¦¬ë·° ì¶”ì¶œ - í¸ì˜ í•¨ìˆ˜"""
    extractor = get_extractor()
    return extractor.extract_review(url, shop_name)

def test_extractor_capability() -> dict:
    """ì¶”ì¶œê¸° ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    extractor = get_extractor()
    return extractor.test_extraction_capability()

# í…ŒìŠ¤íŠ¸ìš© í•¨ìˆ˜
def test_real_extraction():
    """ì‹¤ì œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸"""
    test_urls = [
        {
            "url": "https://m.place.naver.com/my/review/68affe6981fb5b79934cd611?v=2",
            "type": "direct",
            "shop_name": None
        },
        {
            "url": "https://naver.me/5jBm0HYx",
            "type": "list", 
            "shop_name": "ë¯¸ë¯¸í•œ ìƒŒë“œê¹Œì¸ "
        }
    ]
    
    results = []
    extractor = get_extractor()
    
    print("ğŸ§ª ì‹¤ì œ ì¶”ì¶œ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print(f"ì¶”ì¶œê¸° ìƒíƒœ: {extractor.test_extraction_capability()}")
    
    for test in test_urls:
        print(f"\nğŸ“ í…ŒìŠ¤íŠ¸: {test['type']} - {test['url'][:50]}...")
        
        review_text, receipt_date, metadata = extractor.extract_review(
            test['url'], test.get('shop_name')
        )
        
        result = {
            "url": test['url'],
            "type": test['type'],
            "review_text": review_text[:100] + "..." if len(review_text) > 100 else review_text,
            "receipt_date": receipt_date,
            "metadata": metadata
        }
        
        results.append(result)
        print(f"âœ… ê²°ê³¼: {result}")
    
    return results

if __name__ == "__main__":
    # ì§ì ‘ ì‹¤í–‰ì‹œ í…ŒìŠ¤íŠ¸
    test_real_extraction()